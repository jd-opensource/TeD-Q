{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed7dc858",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Qubit rotation\n",
    "\n",
    "This example shows the basic operation of machine learning framework with a quantum device. A qubit is initilized with a arbitary Rx and Ry rotation and the target state is pure |1> state. After several steps of the iteration. The rotation angle of Rx and Ry will converge to 0 and pi. \n",
    "\n",
    "### About this example\n",
    "The example contains the model compiled with three different configurations of backends and interfaces -- JAX backend, JAX backend with pytorch interface and pytorch backend.\n",
    "The example also shows how to use state vector propagation mode and tensor network contraction mode. And two methods for obtaining gradient -- back propagation and parameter shift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496147a8",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2359d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tedq as qai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bda454",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define the quantum model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88114ab",
   "metadata": {},
   "source": [
    "### Define the circuit with TeD-Q framework\n",
    "#### (Remember, if you have multiple measurements, all the measurement results should has the same shape!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "391463f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantum circuit\n",
    "def circuitDef(params):\n",
    "    qai.RX(params[0], qubits=[0])\n",
    "    qai.RY(params[1], qubits=[0])\n",
    "    return qai.expval(qai.PauliZ(qubits=[0]))\n",
    "\n",
    "number_of_qubits = 1\n",
    "parameter_shapes = [(2,)]\n",
    "\n",
    "# Quantum circuit construction\n",
    "circuit = qai.Circuit(circuitDef, number_of_qubits, parameter_shapes = parameter_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb24529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization of the quantum circuit\n",
    "drawer = qai.matplotlib_drawer(circuit)\n",
    "drawer.draw_circuit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfa98e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Circuit compiled with JAX backend\n",
    "Gradient will obtain from backpropagation by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5943f0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### state vector propagation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7315fe6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log10(flops) of this quantum circuit:   1.079181246051244\n"
     ]
    }
   ],
   "source": [
    "my_compilecircuit = circuit.compilecircuit(backend=\"jax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b990a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### tensor network contraction mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081917e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Use CoTenGra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55658bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# slicing_opts = {'target_size': 2**28}\n",
    "# hyper_opt = {'methods':['kahypar'], 'max_time':120, 'max_repeats':12, 'progbar':True, 'minimize':'flops', 'parallel':True, 'slicing_opts':slicing_opts}\n",
    "# import cotengra as ctg\n",
    "# my_compilecircuit = circuit.compilecircuit(backend=\"jax\", use_cotengra=ctg, hyper_opt = hyper_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afc1347",
   "metadata": {},
   "source": [
    "#### Use JDtensorPath (Suggested)\n",
    "1. 'target_num_slices' is useful if you want to do the contraction in parallel, it will devide the tensor network into pieces and then calculat them in parallel\n",
    "2. 'math_repeats' means how many times are going to run JDtensorPath to find a best contraction path\n",
    "3. 'search_parallel' means to run the JDtensorPath in parallel, True means to use all the CPUs, integer number means to use that number of CPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ee930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log10(flops) of this quantum circuit:   1.079181246051244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 23:42:42,899\tINFO services.py:1263 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log2(size) before slicing:  2.0000000000360676\n",
      "log10(flops) before removed:    1.5910646070276129\n"
     ]
    }
   ],
   "source": [
    "from jdtensorpath import JDOptTN as jdopttn\n",
    "slicing_opts = {'target_size':2**28, 'repeats':500, 'target_num_slices':None, 'contract_parallel':False}\n",
    "hyper_opt = {'methods':['kahypar'], 'max_time':120, 'max_repeats':12, 'search_parallel':True, 'slicing_opts':slicing_opts}\n",
    "my_compilecircuit = circuit.compilecircuit(backend=\"jax\", use_jdopttn=jdopttn, hyper_opt = hyper_opt, tn_simplify = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1943e7f",
   "metadata": {},
   "source": [
    "### Define cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b076eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(*params):\n",
    "    return my_compilecircuit(*params)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ee628f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.9998675, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_params = (0.011, 0.012)\n",
    "cost(*new_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac31e0",
   "metadata": {},
   "source": [
    "### Define optimizer\n",
    "TeD-Q built-in optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad7283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer = qai.GradientDescentOptimizer(cost, [0, 1], 0.4, interface=\"jax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a24b2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "034de72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after step     5:  0.9961779\n",
      "Parameters after step     5: (DeviceArray(0.05911537, dtype=float32), DeviceArray(0.0644936, dtype=float32))\n",
      "Cost after step    10:  0.8974943\n",
      "Parameters after step    10: (DeviceArray(0.31109664, dtype=float32), DeviceArray(0.34002095, dtype=float32))\n",
      "Cost after step    15:  0.1440489\n",
      "Parameters after step    15: (DeviceArray(1.0953041, dtype=float32), DeviceArray(1.250686, dtype=float32))\n",
      "Cost after step    20: -0.1536726\n",
      "Parameters after step    20: (DeviceArray(1.1317844, dtype=float32), DeviceArray(1.9407196, dtype=float32))\n",
      "Cost after step    25: -0.9152499\n",
      "Parameters after step    25: (DeviceArray(0.2926945, dtype=float32), DeviceArray(2.8435214, dtype=float32))\n",
      "Cost after step    30: -0.9994047\n",
      "Parameters after step    30: (DeviceArray(0.02419346, dtype=float32), DeviceArray(3.116981, dtype=float32))\n",
      "Cost after step    35: -0.9999964\n",
      "Parameters after step    35: (DeviceArray(0.00188206, dtype=float32), DeviceArray(3.139678, dtype=float32))\n",
      "Cost after step    40: -1.0000000\n",
      "Parameters after step    40: (DeviceArray(0.00014635, dtype=float32), DeviceArray(3.1414437, dtype=float32))\n",
      "Cost after step    45: -1.0000000\n",
      "Parameters after step    45: (DeviceArray(1.1380143e-05, dtype=float32), DeviceArray(3.141581, dtype=float32))\n",
      "Cost after step    50: -1.0000000\n",
      "Parameters after step    50: (DeviceArray(8.8491987e-07, dtype=float32), DeviceArray(3.1415918, dtype=float32))\n",
      "Cost after step    55: -1.0000000\n",
      "Parameters after step    55: (DeviceArray(6.881137e-08, dtype=float32), DeviceArray(3.1415925, dtype=float32))\n",
      "Cost after step    60: -1.0000000\n",
      "Parameters after step    60: (DeviceArray(5.3507714e-09, dtype=float32), DeviceArray(3.1415925, dtype=float32))\n",
      "Cost after step    65: -1.0000000\n",
      "Parameters after step    65: (DeviceArray(4.1607598e-10, dtype=float32), DeviceArray(3.1415925, dtype=float32))\n",
      "Cost after step    70: -1.0000000\n",
      "Parameters after step    70: (DeviceArray(3.2354067e-11, dtype=float32), DeviceArray(3.1415925, dtype=float32))\n",
      "Cost after step    75: -1.0000000\n",
      "Parameters after step    75: (DeviceArray(2.5158521e-12, dtype=float32), DeviceArray(3.1415925, dtype=float32))\n",
      "Cost after step    80: -1.0000000\n",
      "Parameters after step    80: (DeviceArray(1.9563264e-13, dtype=float32), DeviceArray(3.1415925, dtype=float32))\n",
      "Cost after step    85: -1.0000000\n",
      "Parameters after step    85: (DeviceArray(1.5212392e-14, dtype=float32), DeviceArray(3.1415925, dtype=float32))\n",
      "Cost after step    90: -1.0000000\n",
      "Parameters after step    90: (DeviceArray(1.1829154e-15, dtype=float32), DeviceArray(3.1415925, dtype=float32))\n",
      "Cost after step    95: -1.0000000\n",
      "Parameters after step    95: (DeviceArray(9.198349e-17, dtype=float32), DeviceArray(3.1415925, dtype=float32))\n",
      "Cost after step   100: -1.0000000\n",
      "Parameters after step   100: (DeviceArray(7.152636e-18, dtype=float32), DeviceArray(3.1415925, dtype=float32))\n",
      "(DeviceArray(7.152636e-18, dtype=float32), DeviceArray(3.1415925, dtype=float32))\n",
      "-1.0\n",
      "CPU times: user 5.17 s, sys: 39.2 ms, total: 5.21 s\n",
      "Wall time: 5.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_params = (0.011, 0.012)\n",
    "for i in range(100):\n",
    "    new_params = Optimizer.step(*new_params)\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(\"Cost after step {:5d}: {: .7f}\".format(i + 1, cost(*new_params)))\n",
    "        print(\"Parameters after step {:5d}: {}\".format(i + 1, new_params))\n",
    "print(new_params)\n",
    "print(cost(*new_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4d325",
   "metadata": {},
   "source": [
    "### Trained circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25bb4309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainedCircuit = qai.Circuit(circuitDef, 1, new_params)\n",
    "drawer = qai.matplotlib_drawer(trainedCircuit)\n",
    "drawer.full_draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afb1078",
   "metadata": {},
   "source": [
    "# Circuit compiled with JAX backend and pytorch interface\n",
    "Gradient will obtain from backpropagation by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02623248",
   "metadata": {
    "tags": []
   },
   "source": [
    "### state vector propagation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e30c29d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log10(flops) of this quantum circuit:   1.079181246051244\n"
     ]
    }
   ],
   "source": [
    "my_compilecircuit = circuit.compilecircuit(backend=\"jax\", interface=\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b110e6f1-0b27-44cb-9b5f-0b6d7d8a43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "a = jnp.array([[1,2],[2,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "688f9bff-d5a2-4321-bc7c-632afb644a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74909b0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### tensor network contraction mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8f160",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### by using cotengra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c5e2d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log10(flops) of this quantum circuit:   1.079181246051244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log2[SIZE]: 1.00 log10[FLOPs]: 1.64: 100%|██████| 12/12 [00:00<00:00, 46.29it/s]\n"
     ]
    }
   ],
   "source": [
    "slicing_opts = {'target_size': 2**28}\n",
    "hyper_opt = {'methods':['kahypar'], 'max_time':120, 'max_repeats':12, 'progbar':True, 'minimize':'flops', 'parallel':True, 'slicing_opts':slicing_opts}\n",
    "import cotengra as ctg\n",
    "my_compilecircuit = circuit.compilecircuit(backend=\"jax\", use_cotengra=ctg, hyper_opt = hyper_opt, interface=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ddf7a",
   "metadata": {},
   "source": [
    "#### Use JDtensorPath (Suggested)\n",
    "1. 'target_num_slices' is useful if you want to do the contraction in parallel, it will devide the tensor network into pieces and then calculat them in parallel\n",
    "2. 'math_repeats' means how many times are going to run JDtensorPath to find a best contraction path\n",
    "3. 'search_parallel' means to run the JDtensorPath in parallel, True means to use all the CPUs, integer number means to use that number of CPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3cdf211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jdtensorpath import JDOptTN as jdopttn\n",
    "# slicing_opts = {'target_size':2**28, 'repeats':500, 'target_num_slices':None, 'contract_parallel':False}\n",
    "# hyper_opt = {'methods':['kahypar'], 'max_time':120, 'max_repeats':12, 'search_parallel':True, 'slicing_opts':slicing_opts}\n",
    "# my_compilecircuit = circuit.compilecircuit(backend=\"jax\", use_jdopttn=jdopttn, hyper_opt = hyper_opt, tn_simplify = False, interface=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b472f32",
   "metadata": {},
   "source": [
    "### Define cost function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca301b2",
   "metadata": {},
   "source": [
    "### Define cost function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "074a81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(*params):\n",
    "    return my_compilecircuit(*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8c4de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TeD-Q built-in optimizer\n",
    "Optimizer = qai.GradientDescentOptimizer(cost, [0, 1], 0.4, interface=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39a40c",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f2c6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([0.011], requires_grad= True)\n",
    "b = torch.tensor([0.012], requires_grad= True)\n",
    "my_params = (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8da99f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after step     5: tensor([0.9962], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step     5: (tensor([0.0591], requires_grad=True), tensor([0.0645], requires_grad=True))\n",
      "Cost after step    10: tensor([0.8975], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    10: (tensor([0.3111], requires_grad=True), tensor([0.3400], requires_grad=True))\n",
      "Cost after step    15: tensor([0.1440], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    15: (tensor([1.0953], requires_grad=True), tensor([1.2507], requires_grad=True))\n",
      "Cost after step    20: tensor([-0.1537], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    20: (tensor([1.1318], requires_grad=True), tensor([1.9407], requires_grad=True))\n",
      "Cost after step    25: tensor([-0.9152], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    25: (tensor([0.2927], requires_grad=True), tensor([2.8435], requires_grad=True))\n",
      "Cost after step    30: tensor([-0.9994], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    30: (tensor([0.0242], requires_grad=True), tensor([3.1170], requires_grad=True))\n",
      "Cost after step    35: tensor([-1.0000], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    35: (tensor([0.0019], requires_grad=True), tensor([3.1397], requires_grad=True))\n",
      "Cost after step    40: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    40: (tensor([0.0001], requires_grad=True), tensor([3.1414], requires_grad=True))\n",
      "Cost after step    45: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    45: (tensor([1.1380e-05], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "Cost after step    50: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    50: (tensor([8.8492e-07], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "Cost after step    55: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    55: (tensor([6.8811e-08], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "Cost after step    60: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    60: (tensor([5.3508e-09], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "Cost after step    65: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    65: (tensor([4.1608e-10], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "Cost after step    70: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    70: (tensor([3.2354e-11], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "Cost after step    75: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    75: (tensor([2.5159e-12], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "Cost after step    80: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    80: (tensor([1.9563e-13], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "Cost after step    85: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    85: (tensor([1.5212e-14], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "Cost after step    90: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    90: (tensor([1.1829e-15], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "Cost after step    95: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step    95: (tensor([9.1983e-17], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "Cost after step   100: tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "Parameters after step   100: (tensor([7.1526e-18], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "(tensor([7.1526e-18], requires_grad=True), tensor([3.1416], requires_grad=True))\n",
      "tensor([-1.], grad_fn=<PytorchInterfaceBackward>)\n",
      "CPU times: user 7.23 s, sys: 58.2 ms, total: 7.28 s\n",
      "Wall time: 7.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_params = my_params\n",
    "for i in range(100):\n",
    "    new_params = Optimizer.step(*new_params)\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(\"Cost after step {:5d}: {}\".format(i + 1, cost(*new_params)))\n",
    "        print(\"Parameters after step {:5d}: {}\".format(i + 1, new_params))\n",
    "print(new_params)\n",
    "print(cost(*new_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d77df",
   "metadata": {},
   "source": [
    "### Trained circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1c84abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainedCircuit = qai.Circuit(circuitDef, 1, new_params)\n",
    "drawer = qai.matplotlib_drawer(trainedCircuit)\n",
    "drawer.full_draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818bf875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "623ff679",
   "metadata": {},
   "source": [
    "# Circuit compiled with pytorch backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14cac3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Gradient will obtain from backpropagation by default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010154df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### state vector propagation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba72fbd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# my_compilecircuit = circuit.compilecircuit(backend=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6051a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### tensor network contraction mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be7e18",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Use CoTenGra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "094402bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# slicing_opts = {'target_size': 2**28}\n",
    "# hyper_opt = {'methods':['kahypar'], 'max_time':120, 'max_repeats':12, 'progbar':True, 'minimize':'flops', 'parallel':True, 'slicing_opts':slicing_opts}\n",
    "# import cotengra as ctg\n",
    "# my_compilecircuit = circuit.compilecircuit(backend=\"pytorch\", use_cotengra=ctg, hyper_opt = hyper_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba324828",
   "metadata": {},
   "source": [
    "#### Use JDtensorPath (Suggested)\n",
    "1. 'target_num_slices' is useful if you want to do the contraction in parallel, it will devide the tensor network into pieces and then calculat them in parallel\n",
    "2. 'math_repeats' means how many times are going to run JDtensorPath to find a best contraction path\n",
    "3. 'search_parallel' means to run the JDtensorPath in parallel, True means to use all the CPUs, integer number means to use that number of CPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f664e876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log10(flops) of this quantum circuit:   1.079181246051244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 23:43:02,111\tINFO services.py:1263 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log2(size) before slicing:  2.0000000000360676\n",
      "log10(flops) before removed:    1.5910646070276129\n"
     ]
    }
   ],
   "source": [
    "from jdtensorpath import JDOptTN as jdopttn\n",
    "slicing_opts = {'target_size':2**28, 'repeats':500, 'target_num_slices':None, 'contract_parallel':False}\n",
    "hyper_opt = {'methods':['kahypar'], 'max_time':120, 'max_repeats':12, 'search_parallel':True, 'slicing_opts':slicing_opts}\n",
    "my_compilecircuit = circuit.compilecircuit(backend=\"pytorch\", use_jdopttn=jdopttn, hyper_opt = hyper_opt, tn_simplify = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd16662",
   "metadata": {},
   "source": [
    "### Define cost function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67a7ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(*params):\n",
    "    results = my_compilecircuit(*params)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b174b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TeD-Q built-in optimizer\n",
    "Optimizer = qai.GradientDescentOptimizer(cost, [0, 1], 0.4, interface=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c183866",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94747b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([0.54], requires_grad= True)\n",
    "b = torch.tensor([0.12], requires_grad= True)\n",
    "my_params = (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9d49951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after step     1: tensor([0.7261], grad_fn=<StackBackward0>)\n",
      "Parameters after step     1: (tensor([0.7442], requires_grad=True), tensor([0.1611], requires_grad=True))\n",
      "Cost after step     2: tensor([0.5190], grad_fn=<StackBackward0>)\n",
      "Parameters after step     2: (tensor([1.0116], requires_grad=True), tensor([0.2083], requires_grad=True))\n",
      "Cost after step     3: tensor([0.2183], grad_fn=<StackBackward0>)\n",
      "Parameters after step     3: (tensor([1.3434], requires_grad=True), tensor([0.2521], requires_grad=True))\n",
      "Cost after step     4: tensor([-0.1438], grad_fn=<StackBackward0>)\n",
      "Parameters after step     4: (tensor([1.7207], requires_grad=True), tensor([0.2746], requires_grad=True))\n",
      "Cost after step     5: tensor([-0.4893], grad_fn=<StackBackward0>)\n",
      "Parameters after step     5: (tensor([2.1014], requires_grad=True), tensor([0.2584], requires_grad=True))\n",
      "Cost after step     6: tensor([-0.7444], grad_fn=<StackBackward0>)\n",
      "Parameters after step     6: (tensor([2.4350], requires_grad=True), tensor([0.2067], requires_grad=True))\n",
      "Cost after step     7: tensor([-0.8900], grad_fn=<StackBackward0>)\n",
      "Parameters after step     7: (tensor([2.6891], requires_grad=True), tensor([0.1443], requires_grad=True))\n",
      "Cost after step     8: tensor([-0.9571], grad_fn=<StackBackward0>)\n",
      "Parameters after step     8: (tensor([2.8622], requires_grad=True), tensor([0.0925], requires_grad=True))\n",
      "Cost after step     9: tensor([-0.9841], grad_fn=<StackBackward0>)\n",
      "Parameters after step     9: (tensor([2.9720], requires_grad=True), tensor([0.0570], requires_grad=True))\n",
      "Cost after step    10: tensor([-0.9942], grad_fn=<StackBackward0>)\n",
      "Parameters after step    10: (tensor([3.0394], requires_grad=True), tensor([0.0345], requires_grad=True))\n",
      "Cost after step    11: tensor([-0.9979], grad_fn=<StackBackward0>)\n",
      "Parameters after step    11: (tensor([3.0802], requires_grad=True), tensor([0.0208], requires_grad=True))\n",
      "Cost after step    12: tensor([-0.9992], grad_fn=<StackBackward0>)\n",
      "Parameters after step    12: (tensor([3.1047], requires_grad=True), tensor([0.0125], requires_grad=True))\n",
      "Cost after step    13: tensor([-0.9997], grad_fn=<StackBackward0>)\n",
      "Parameters after step    13: (tensor([3.1195], requires_grad=True), tensor([0.0075], requires_grad=True))\n",
      "Cost after step    14: tensor([-0.9999], grad_fn=<StackBackward0>)\n",
      "Parameters after step    14: (tensor([3.1283], requires_grad=True), tensor([0.0045], requires_grad=True))\n",
      "Cost after step    15: tensor([-1.0000], grad_fn=<StackBackward0>)\n",
      "Parameters after step    15: (tensor([3.1336], requires_grad=True), tensor([0.0027], requires_grad=True))\n",
      "Cost after step    16: tensor([-1.0000], grad_fn=<StackBackward0>)\n",
      "Parameters after step    16: (tensor([3.1368], requires_grad=True), tensor([0.0016], requires_grad=True))\n",
      "Cost after step    17: tensor([-1.0000], grad_fn=<StackBackward0>)\n",
      "Parameters after step    17: (tensor([3.1387], requires_grad=True), tensor([0.0010], requires_grad=True))\n",
      "Cost after step    18: tensor([-1.0000], grad_fn=<StackBackward0>)\n",
      "Parameters after step    18: (tensor([3.1399], requires_grad=True), tensor([0.0006], requires_grad=True))\n",
      "Cost after step    19: tensor([-1.0000], grad_fn=<StackBackward0>)\n",
      "Parameters after step    19: (tensor([3.1406], requires_grad=True), tensor([0.0004], requires_grad=True))\n",
      "Cost after step    20: tensor([-1.0000], grad_fn=<StackBackward0>)\n",
      "Parameters after step    20: (tensor([3.1410], requires_grad=True), tensor([0.0002], requires_grad=True))\n",
      "Cost after step    21: tensor([-1.0000], grad_fn=<StackBackward0>)\n",
      "Parameters after step    21: (tensor([3.1412], requires_grad=True), tensor([0.0001], requires_grad=True))\n",
      "Cost after step    22: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    22: (tensor([3.1414], requires_grad=True), tensor([7.5626e-05], requires_grad=True))\n",
      "Cost after step    23: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    23: (tensor([3.1415], requires_grad=True), tensor([4.5375e-05], requires_grad=True))\n",
      "Cost after step    24: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    24: (tensor([3.1415], requires_grad=True), tensor([2.7225e-05], requires_grad=True))\n",
      "Cost after step    25: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    25: (tensor([3.1415], requires_grad=True), tensor([1.6335e-05], requires_grad=True))\n",
      "Cost after step    26: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    26: (tensor([3.1416], requires_grad=True), tensor([9.8011e-06], requires_grad=True))\n",
      "Cost after step    27: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    27: (tensor([3.1416], requires_grad=True), tensor([5.8807e-06], requires_grad=True))\n",
      "Cost after step    28: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    28: (tensor([3.1416], requires_grad=True), tensor([3.5284e-06], requires_grad=True))\n",
      "Cost after step    29: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    29: (tensor([3.1416], requires_grad=True), tensor([2.1170e-06], requires_grad=True))\n",
      "Cost after step    30: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    30: (tensor([3.1416], requires_grad=True), tensor([1.2702e-06], requires_grad=True))\n",
      "Cost after step    31: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    31: (tensor([3.1416], requires_grad=True), tensor([7.6213e-07], requires_grad=True))\n",
      "Cost after step    32: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    32: (tensor([3.1416], requires_grad=True), tensor([4.5728e-07], requires_grad=True))\n",
      "Cost after step    33: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    33: (tensor([3.1416], requires_grad=True), tensor([2.7437e-07], requires_grad=True))\n",
      "Cost after step    34: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    34: (tensor([3.1416], requires_grad=True), tensor([1.6462e-07], requires_grad=True))\n",
      "Cost after step    35: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    35: (tensor([3.1416], requires_grad=True), tensor([9.8772e-08], requires_grad=True))\n",
      "Cost after step    36: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    36: (tensor([3.1416], requires_grad=True), tensor([5.9263e-08], requires_grad=True))\n",
      "Cost after step    37: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    37: (tensor([3.1416], requires_grad=True), tensor([3.5558e-08], requires_grad=True))\n",
      "Cost after step    38: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    38: (tensor([3.1416], requires_grad=True), tensor([2.1335e-08], requires_grad=True))\n",
      "Cost after step    39: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    39: (tensor([3.1416], requires_grad=True), tensor([1.2801e-08], requires_grad=True))\n",
      "Cost after step    40: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    40: (tensor([3.1416], requires_grad=True), tensor([7.6805e-09], requires_grad=True))\n",
      "Cost after step    41: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    41: (tensor([3.1416], requires_grad=True), tensor([4.6083e-09], requires_grad=True))\n",
      "Cost after step    42: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    42: (tensor([3.1416], requires_grad=True), tensor([2.7650e-09], requires_grad=True))\n",
      "Cost after step    43: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    43: (tensor([3.1416], requires_grad=True), tensor([1.6590e-09], requires_grad=True))\n",
      "Cost after step    44: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    44: (tensor([3.1416], requires_grad=True), tensor([9.9540e-10], requires_grad=True))\n",
      "Cost after step    45: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    45: (tensor([3.1416], requires_grad=True), tensor([5.9724e-10], requires_grad=True))\n",
      "Cost after step    46: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    46: (tensor([3.1416], requires_grad=True), tensor([3.5834e-10], requires_grad=True))\n",
      "Cost after step    47: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    47: (tensor([3.1416], requires_grad=True), tensor([2.1501e-10], requires_grad=True))\n",
      "Cost after step    48: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    48: (tensor([3.1416], requires_grad=True), tensor([1.2900e-10], requires_grad=True))\n",
      "Cost after step    49: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    49: (tensor([3.1416], requires_grad=True), tensor([7.7402e-11], requires_grad=True))\n",
      "Cost after step    50: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    50: (tensor([3.1416], requires_grad=True), tensor([4.6441e-11], requires_grad=True))\n",
      "Cost after step    51: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    51: (tensor([3.1416], requires_grad=True), tensor([2.7865e-11], requires_grad=True))\n",
      "Cost after step    52: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    52: (tensor([3.1416], requires_grad=True), tensor([1.6719e-11], requires_grad=True))\n",
      "Cost after step    53: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    53: (tensor([3.1416], requires_grad=True), tensor([1.0031e-11], requires_grad=True))\n",
      "Cost after step    54: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    54: (tensor([3.1416], requires_grad=True), tensor([6.0188e-12], requires_grad=True))\n",
      "Cost after step    55: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    55: (tensor([3.1416], requires_grad=True), tensor([3.6113e-12], requires_grad=True))\n",
      "Cost after step    56: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    56: (tensor([3.1416], requires_grad=True), tensor([2.1668e-12], requires_grad=True))\n",
      "Cost after step    57: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    57: (tensor([3.1416], requires_grad=True), tensor([1.3001e-12], requires_grad=True))\n",
      "Cost after step    58: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    58: (tensor([3.1416], requires_grad=True), tensor([7.8004e-13], requires_grad=True))\n",
      "Cost after step    59: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    59: (tensor([3.1416], requires_grad=True), tensor([4.6802e-13], requires_grad=True))\n",
      "Cost after step    60: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    60: (tensor([3.1416], requires_grad=True), tensor([2.8081e-13], requires_grad=True))\n",
      "Cost after step    61: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    61: (tensor([3.1416], requires_grad=True), tensor([1.6849e-13], requires_grad=True))\n",
      "Cost after step    62: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    62: (tensor([3.1416], requires_grad=True), tensor([1.0109e-13], requires_grad=True))\n",
      "Cost after step    63: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    63: (tensor([3.1416], requires_grad=True), tensor([6.0656e-14], requires_grad=True))\n",
      "Cost after step    64: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    64: (tensor([3.1416], requires_grad=True), tensor([3.6393e-14], requires_grad=True))\n",
      "Cost after step    65: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    65: (tensor([3.1416], requires_grad=True), tensor([2.1836e-14], requires_grad=True))\n",
      "Cost after step    66: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    66: (tensor([3.1416], requires_grad=True), tensor([1.3102e-14], requires_grad=True))\n",
      "Cost after step    67: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    67: (tensor([3.1416], requires_grad=True), tensor([7.8610e-15], requires_grad=True))\n",
      "Cost after step    68: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    68: (tensor([3.1416], requires_grad=True), tensor([4.7166e-15], requires_grad=True))\n",
      "Cost after step    69: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    69: (tensor([3.1416], requires_grad=True), tensor([2.8299e-15], requires_grad=True))\n",
      "Cost after step    70: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    70: (tensor([3.1416], requires_grad=True), tensor([1.6980e-15], requires_grad=True))\n",
      "Cost after step    71: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    71: (tensor([3.1416], requires_grad=True), tensor([1.0188e-15], requires_grad=True))\n",
      "Cost after step    72: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    72: (tensor([3.1416], requires_grad=True), tensor([6.1127e-16], requires_grad=True))\n",
      "Cost after step    73: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    73: (tensor([3.1416], requires_grad=True), tensor([3.6676e-16], requires_grad=True))\n",
      "Cost after step    74: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    74: (tensor([3.1416], requires_grad=True), tensor([2.2006e-16], requires_grad=True))\n",
      "Cost after step    75: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    75: (tensor([3.1416], requires_grad=True), tensor([1.3203e-16], requires_grad=True))\n",
      "Cost after step    76: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    76: (tensor([3.1416], requires_grad=True), tensor([7.9220e-17], requires_grad=True))\n",
      "Cost after step    77: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    77: (tensor([3.1416], requires_grad=True), tensor([4.7532e-17], requires_grad=True))\n",
      "Cost after step    78: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    78: (tensor([3.1416], requires_grad=True), tensor([2.8519e-17], requires_grad=True))\n",
      "Cost after step    79: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    79: (tensor([3.1416], requires_grad=True), tensor([1.7112e-17], requires_grad=True))\n",
      "Cost after step    80: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    80: (tensor([3.1416], requires_grad=True), tensor([1.0267e-17], requires_grad=True))\n",
      "Cost after step    81: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    81: (tensor([3.1416], requires_grad=True), tensor([6.1602e-18], requires_grad=True))\n",
      "Cost after step    82: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    82: (tensor([3.1416], requires_grad=True), tensor([3.6961e-18], requires_grad=True))\n",
      "Cost after step    83: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    83: (tensor([3.1416], requires_grad=True), tensor([2.2177e-18], requires_grad=True))\n",
      "Cost after step    84: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    84: (tensor([3.1416], requires_grad=True), tensor([1.3306e-18], requires_grad=True))\n",
      "Cost after step    85: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    85: (tensor([3.1416], requires_grad=True), tensor([7.9836e-19], requires_grad=True))\n",
      "Cost after step    86: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    86: (tensor([3.1416], requires_grad=True), tensor([4.7902e-19], requires_grad=True))\n",
      "Cost after step    87: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    87: (tensor([3.1416], requires_grad=True), tensor([2.8741e-19], requires_grad=True))\n",
      "Cost after step    88: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    88: (tensor([3.1416], requires_grad=True), tensor([1.7245e-19], requires_grad=True))\n",
      "Cost after step    89: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    89: (tensor([3.1416], requires_grad=True), tensor([1.0347e-19], requires_grad=True))\n",
      "Cost after step    90: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    90: (tensor([3.1416], requires_grad=True), tensor([6.2080e-20], requires_grad=True))\n",
      "Cost after step    91: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    91: (tensor([3.1416], requires_grad=True), tensor([3.7248e-20], requires_grad=True))\n",
      "Cost after step    92: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    92: (tensor([3.1416], requires_grad=True), tensor([2.2349e-20], requires_grad=True))\n",
      "Cost after step    93: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    93: (tensor([3.1416], requires_grad=True), tensor([1.3409e-20], requires_grad=True))\n",
      "Cost after step    94: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    94: (tensor([3.1416], requires_grad=True), tensor([8.0456e-21], requires_grad=True))\n",
      "Cost after step    95: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    95: (tensor([3.1416], requires_grad=True), tensor([4.8274e-21], requires_grad=True))\n",
      "Cost after step    96: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    96: (tensor([3.1416], requires_grad=True), tensor([2.8964e-21], requires_grad=True))\n",
      "Cost after step    97: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    97: (tensor([3.1416], requires_grad=True), tensor([1.7379e-21], requires_grad=True))\n",
      "Cost after step    98: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    98: (tensor([3.1416], requires_grad=True), tensor([1.0427e-21], requires_grad=True))\n",
      "Cost after step    99: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    99: (tensor([3.1416], requires_grad=True), tensor([6.2563e-22], requires_grad=True))\n",
      "Cost after step   100: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   100: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "(tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "tensor([-1.], grad_fn=<StackBackward0>)\n",
      "CPU times: user 541 ms, sys: 32.6 ms, total: 574 ms\n",
      "Wall time: 558 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_params = my_params\n",
    "for i in range(100):\n",
    "    new_params = Optimizer.step(*new_params)\n",
    "    if (i + 1) % 1 == 0:\n",
    "        print(\"Cost after step {:5d}: {}\".format(i + 1, cost(*new_params)))\n",
    "        print(\"Parameters after step {:5d}: {}\".format(i + 1, new_params))\n",
    "print(new_params)\n",
    "print(cost(*new_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27bd2108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized rotation angles: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimized rotation angles: {}\".format(new_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80b0fef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: tensor([-1.], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost: {}\".format(cost(*new_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf430d",
   "metadata": {},
   "source": [
    "### Using backend's optimizer and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bef61d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after step     5: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step     5: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    10: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    10: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    15: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    15: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    20: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    20: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    25: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    25: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    30: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    30: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    35: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    35: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    40: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    40: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    45: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    45: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    50: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    50: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    55: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    55: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    60: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    60: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    65: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    65: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    70: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    70: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    75: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    75: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    80: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    80: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    85: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    85: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    90: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    90: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step    95: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step    95: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   100: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   100: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   105: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   105: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   110: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   110: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   115: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   115: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   120: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   120: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   125: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   125: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   130: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   130: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   135: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   135: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   140: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   140: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   145: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   145: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   150: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   150: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   155: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   155: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   160: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   160: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   165: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   165: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   170: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   170: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   175: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   175: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   180: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   180: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   185: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   185: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   190: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   190: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   195: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   195: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   200: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   200: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   205: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   205: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   210: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   210: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   215: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   215: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   220: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   220: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   225: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   225: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   230: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   230: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   235: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   235: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   240: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   240: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   245: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   245: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   250: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   250: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   255: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   255: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   260: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   260: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   265: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   265: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   270: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   270: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   275: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   275: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   280: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   280: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   285: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   285: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   290: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   290: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   295: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   295: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   300: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   300: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   305: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   305: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   310: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   310: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   315: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   315: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   320: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   320: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   325: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   325: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   330: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   330: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   335: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   335: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   340: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   340: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   345: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   345: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   350: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   350: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   355: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   355: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   360: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   360: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   365: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   365: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   370: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   370: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   375: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   375: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   380: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   380: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   385: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   385: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   390: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   390: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   395: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   395: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   400: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   400: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   405: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   405: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   410: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   410: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   415: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   415: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   420: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   420: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   425: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   425: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   430: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   430: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   435: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   435: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   440: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   440: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   445: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   445: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   450: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   450: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   455: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   455: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   460: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   460: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   465: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   465: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   470: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   470: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   475: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   475: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   480: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   480: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   485: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   485: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   490: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   490: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   495: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   495: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "Cost after step   500: tensor([-1.], grad_fn=<StackBackward0>)\n",
      "Parameters after step   500: (tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "(tensor([3.1416], requires_grad=True), tensor([3.7538e-22], requires_grad=True))\n",
      "tensor([-1.], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.Adam([a, b], lr=0.1)\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    #print(b.grad)\n",
    "    loss = cost(*my_params)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(\"Cost after step {:5d}: {}\".format(i + 1, cost(*new_params)))\n",
    "        print(\"Parameters after step {:5d}: {}\".format(i + 1, new_params))\n",
    "print(new_params)\n",
    "print(cost(*new_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd22cefb",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Obtain gradient by parameter shift method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3498a55c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### state vector propagation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2b66344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# my_compilecircuit = circuit.compilecircuit(backend=\"pytorch\", diff_method = \"param_shift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ea829",
   "metadata": {
    "tags": []
   },
   "source": [
    "### tensor network contraction mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb525f46",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Use CoTenGra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08643b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# slicing_opts = {'target_size': 2**28}\n",
    "# hyper_opt = {'methods':['kahypar'], 'max_time':120, 'max_repeats':12, 'progbar':True, 'minimize':'flops', 'parallel':True, 'slicing_opts':slicing_opts}\n",
    "# import cotengra as ctg\n",
    "# my_compilecircuit = circuit.compilecircuit(backend=\"pytorch\", use_cotengra=ctg, hyper_opt = hyper_opt, diff_method = \"param_shift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bdaad",
   "metadata": {},
   "source": [
    "#### Use JDtensorPath (Suggested)\n",
    "1. 'target_num_slices' is useful if you want to do the contraction in parallel, it will devide the tensor network into pieces and then calculat them in parallel\n",
    "2. 'math_repeats' means how many times are going to run JDtensorPath to find a best contraction path\n",
    "3. 'search_parallel' means to run the JDtensorPath in parallel, True means to use all the CPUs, integer number means to use that number of CPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d051a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log10(flops) of this quantum circuit:   1.079181246051244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 23:43:08,416\tINFO services.py:1263 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log2(size) before slicing:  2.0000000000360676\n",
      "log10(flops) before removed:    1.5910646070276129\n"
     ]
    }
   ],
   "source": [
    "from jdtensorpath import JDOptTN as jdopttn\n",
    "slicing_opts = {'target_size':2**28, 'repeats':500, 'target_num_slices':None, 'contract_parallel':False}\n",
    "hyper_opt = {'methods':['kahypar'], 'max_time':120, 'max_repeats':12, 'search_parallel':True, 'slicing_opts':slicing_opts}\n",
    "my_compilecircuit = circuit.compilecircuit(backend=\"pytorch\", use_jdopttn=jdopttn, hyper_opt = hyper_opt, tn_simplify = False, diff_method = \"param_shift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c587a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define cost function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4aca28db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(*params):\n",
    "    results = my_compilecircuit(*params)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ad4035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TeD-Q built-in optimizer\n",
    "Optimizer = qai.GradientDescentOptimizer(cost, [0, 1], 0.4, interface=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebad5be",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6928ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([0.54], requires_grad= True)\n",
    "b = torch.tensor([0.12], requires_grad= True)\n",
    "my_params = (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02d8be77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after step     5: tensor([-0.4893], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step     5: (tensor([2.1014], requires_grad=True), tensor([0.2584], requires_grad=True))\n",
      "Cost after step    10: tensor([-0.9942], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    10: (tensor([3.0394], requires_grad=True), tensor([0.0345], requires_grad=True))\n",
      "Cost after step    15: tensor([-1.0000], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    15: (tensor([3.1336], requires_grad=True), tensor([0.0027], requires_grad=True))\n",
      "Cost after step    20: tensor([-1.0000], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    20: (tensor([3.1410], requires_grad=True), tensor([0.0002], requires_grad=True))\n",
      "Cost after step    25: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    25: (tensor([3.1415], requires_grad=True), tensor([1.6321e-05], requires_grad=True))\n",
      "Cost after step    30: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    30: (tensor([3.1416], requires_grad=True), tensor([1.2409e-06], requires_grad=True))\n",
      "Cost after step    35: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    35: (tensor([3.1416], requires_grad=True), tensor([9.0574e-08], requires_grad=True))\n",
      "Cost after step    40: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    40: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    45: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    45: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    50: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    50: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    55: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    55: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    60: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    60: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    65: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    65: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    70: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    70: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    75: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    75: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    80: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    80: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    85: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    85: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    90: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    90: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    95: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    95: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   100: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   100: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "(tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "CPU times: user 489 ms, sys: 3.73 ms, total: 493 ms\n",
      "Wall time: 488 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_params = my_params\n",
    "for i in range(100):\n",
    "    new_params = Optimizer.step(*new_params)\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(\"Cost after step {:5d}: {}\".format(i + 1, cost(*new_params)))\n",
    "        print(\"Parameters after step {:5d}: {}\".format(i + 1, new_params))\n",
    "print(new_params)\n",
    "print(cost(*new_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c50308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized rotation angles: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimized rotation angles: {}\".format(new_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee9fa229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost: {}\".format(cost(*new_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0df62",
   "metadata": {},
   "source": [
    "### Using backend's optimizer and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9abfb309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after step     5: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step     5: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    10: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    10: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    15: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    15: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    20: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    20: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    25: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    25: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    30: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    30: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    35: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    35: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    40: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    40: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    45: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    45: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    50: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    50: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    55: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    55: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    60: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    60: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    65: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    65: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    70: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    70: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    75: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    75: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    80: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    80: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    85: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    85: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    90: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    90: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step    95: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step    95: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   100: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   100: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   105: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   105: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   110: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   110: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   115: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   115: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   120: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   120: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   125: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   125: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   130: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   130: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   135: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   135: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   140: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   140: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   145: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   145: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   150: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   150: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   155: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   155: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   160: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   160: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   165: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   165: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   170: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   170: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   175: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   175: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   180: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   180: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   185: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   185: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   190: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   190: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   195: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   195: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   200: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   200: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   205: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   205: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   210: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   210: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   215: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   215: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   220: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   220: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   225: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   225: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   230: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   230: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   235: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   235: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   240: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   240: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   245: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   245: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   250: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   250: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   255: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   255: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   260: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   260: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   265: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   265: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   270: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   270: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   275: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   275: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   280: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   280: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   285: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   285: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   290: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   290: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   295: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   295: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   300: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   300: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   305: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   305: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   310: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   310: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   315: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   315: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   320: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   320: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   325: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   325: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   330: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   330: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   335: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   335: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   340: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   340: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   345: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   345: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   350: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   350: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   355: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   355: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   360: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   360: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   365: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   365: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   370: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   370: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   375: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   375: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   380: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   380: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   385: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   385: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   390: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   390: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   395: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   395: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   400: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   400: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   405: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   405: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   410: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   410: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   415: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   415: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   420: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   420: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   425: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   425: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   430: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   430: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   435: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   435: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   440: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   440: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   445: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   445: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   450: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   450: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   455: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   455: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   460: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   460: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   465: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   465: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   470: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   470: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   475: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   475: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   480: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   480: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   485: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   485: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   490: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   490: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   495: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   495: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "Cost after step   500: tensor([-1.], grad_fn=<TorchExecuteBackward>)\n",
      "Parameters after step   500: (tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "(tensor([3.1416], requires_grad=True), tensor([4.2890e-08], requires_grad=True))\n",
      "tensor([-1.], grad_fn=<TorchExecuteBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "optimizer = optim.Adam([a, b], lr=0.1)\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    #print(b.grad)\n",
    "    loss = cost(*my_params)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(\"Cost after step {:5d}: {}\".format(i + 1, cost(*new_params)))\n",
    "        print(\"Parameters after step {:5d}: {}\".format(i + 1, new_params))\n",
    "print(new_params)\n",
    "print(cost(*new_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d601fee0",
   "metadata": {},
   "source": [
    "### Trained circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0f485a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainedCircuit = qai.Circuit(circuitDef, 1, new_params)\n",
    "drawer = qai.matplotlib_drawer(trainedCircuit)\n",
    "drawer.full_draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8dec7c-da9c-4566-8379-2e6cde05133d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6874b61b-88ad-464e-83a8-267d17306af0",
   "metadata": {},
   "source": [
    "# Real quantum computer hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc57e3-65c1-4fd8-a832-63ac00f2cc59",
   "metadata": {},
   "source": [
    "#### pytorch interface, parameter shift gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87241d76-75b3-4fd1-a29c-248922f1f136",
   "metadata": {},
   "source": [
    "#### now only support 1 qubit PauliZ measurement, will be upgrade later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc0c0e-f692-4c1f-ba79-c38a2b197d66",
   "metadata": {},
   "source": [
    "#### check your jobs here: https://quantum-computing.ibm.com/jobs?jobs=circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d6ef3a-28a5-4e2c-9d25-4a775dcb8086",
   "metadata": {},
   "source": [
    "#### caution, this is extremely slow since so many people are queuing for IBMQ free quantum computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cea7c-9157-432b-a05f-99f35be222f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import IBMQ\n",
    "IBMQ.enable_account('your IBMQ token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96536b-04fc-4225-8d3c-d9994ffdc0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_compilecircuit = circuit.compilecircuit(backend=\"IBMQ_hardware\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba326f6-ddf6-4c71-8ea4-77b3ad46a80d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define cost function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98b2b9c-05c7-4f9c-a0a6-ef2512c74c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(*params):\n",
    "    results = my_compilecircuit(*params)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4932e0a7-27b8-4946-b8bc-4fa8d680d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TeD-Q built-in optimizer\n",
    "Optimizer = qai.GradientDescentOptimizer(cost, [0, 1], 0.8, interface=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795a7ea-47d5-4883-9811-47bda76a808e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc73a1f-3ebd-4c19-8b76-48bf19a05fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([0.54], requires_grad= True)\n",
    "b = torch.tensor([0.12], requires_grad= True)\n",
    "my_params = (a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73ad34-90dc-4282-80b3-35d28623c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost(*my_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d513cce-589b-4bab-b2c3-e5942ccf1cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_params = my_params\n",
    "for i in range(10):\n",
    "    new_params = Optimizer.step(*new_params)\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(\"Parameters after step {:5d}: {}\".format(i + 1, new_params))\n",
    "print(new_params)\n",
    "print(cost(*new_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3bc5b-da43-4efc-9661-9dabfcbd6ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
